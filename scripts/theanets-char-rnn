#!/usr/bin/env python

import click
import logging
import numpy as np
import theanets

@click.command()
@click.option('-d', '--data', multiple=True, metavar='FILE',
              help='load text from FILE')
@click.option('-t', '--time', default=100, type=int, metavar='T',
              help='train on sequences of T characters')
@click.option('-a', '--alphabet', default='', metavar='CHARS',
              help='use CHARS for alphabet; defaults to all chars in text')
@click.option('-A', '--exclude-alphabet', default='', metavar='CHARS',
              help='discard CHARS from alphabet')
@click.option('-l', '--layers', multiple=True, type=int, default=[100], metavar='N',
              help='construct a network with layers of size N1, N2, ...')
@click.option('-L', '--layer-type', default='lstm', metavar='{rnn|gru|lstm|clockwork}',
              help='construct a network with this RNN layer type')
@click.option('-g', '--activation', default='relu', metavar='FUNC',
              help='function for hidden unit activations')
@click.option('-O', '--algorithm', default=['nag'], multiple=True, metavar='ALGO',
               help='train with the given optimization algorithm(s)')
@click.option('-p', '--patience', type=int, default=4, metavar='N',
               help='stop training if less than --min-improvement for N validations')
@click.option('-v', '--validate-every', type=int, default=10, metavar='N',
               help='validate the model every N updates')
@click.option('-b', '--batch-size', type=int, default=64, metavar='N',
               help='train with mini-batches of size N')
@click.option('-B', '--train-batches', type=int, metavar='N',
               help='use at most N batches during gradient computations')
@click.option('-V', '--valid-batches', type=int, metavar='N',
               help='use at most N batches during validation')
@click.option('-i', '--min-improvement', type=float, default=0, metavar='R',
               help='train until relative improvement is less than R')
@click.option('-x', '--max-gradient-norm', type=float, default=1, metavar='V',
               help='clip gradient norm to the interval [0, V]')
@click.option('-r', '--learning-rate', type=float, default=1e-4, metavar='V',
               help='train the network with a learning rate of V')
@click.option('-m', '--momentum', type=float, default=0.9, metavar='V',
               help='train the network with momentum of V')
@click.option('-n', '--nesterov/--no-nesterov', help='use Nesterov momentum')
@click.option('-s', '--save-progress', metavar='FILE',
               help='save the model periodically to FILE')
@click.option('-S', '--save-every', type=float, default=0, metavar='N',
               help='save the model every N iterations or -N minutes')
@click.option('--input-noise', type=float, default=0, metavar='S',
               help='add noise to network inputs drawn from N(0, S)')
@click.option('--input-dropouts', type=float, default=0, metavar='R',
               help='randomly set fraction R of input activations to 0')
@click.option('--hidden-noise', type=float, default=0, metavar='S',
               help='add noise to hidden activations drawn from N(0, S)')
@click.option('--hidden-dropouts', type=float, default=0, metavar='R',
               help='randomly set fraction R of hidden activations to 0')
@click.option('--hidden-l1', type=float, default=0, metavar='K',
               help='regularize hidden activity with K on the L1 term')
@click.option('--hidden-l2', type=float, default=0, metavar='K',
               help='regularize hidden activity with K on the L2 term')
@click.option('--weight-l1', type=float, default=0, metavar='K',
               help='regularize network weights with K on the L1 term')
@click.option('--weight-l2', type=float, default=0, metavar='K',
               help='regularize network weights with K on the L2 term')
@click.option('--rms-halflife', type=float, default=5, metavar='N',
               help='use a half-life of N for RMS exponential moving averages')
@click.option('--rms-regularizer', type=float, default=1e-8, metavar='N',
               help='regularize RMS exponential moving averages by N')
def main(**kwargs):
    corpus = []
    for f in kwargs['data']:
        corpus.append(open(f).read())
        logging.info('%s: loaded training document', f)
    logging.info('loaded %d training documents', len(corpus))

    alpha = set(kwargs['alphabet'])
    if not alpha:
        for c in corpus:
            alpha |= set(c)
    alpha -= set(kwargs['exclude_alphabet'])
    alpha = sorted(alpha)
    logging.info('character alphabet: %s', alpha)

    # encode document chars as integer alphabet index values.
    encoded = [np.array([alpha.index(c) for c in doc]) for doc in corpus]

    def batch():
        T, B = kwargs['time'], kwargs['batch_size']
        inputs = np.zeros((T, B, len(alpha)), 'f')
        outputs = np.zeros((T, B), 'i')
        enc = np.random.choice(encoded)
        for b in range(B):
            o = np.random.randint(len(enc) - T - 1)
            inputs[np.arange(T), b, enc[o:o+T]] = 1
            outputs[np.arange(T), b] = enc[o+1:o+T+1]
        return [inputs, outputs]

    layers = [len(alpha)]
    for l in kwargs['layers']:
        layers.append(dict(size=l,
                           form=kwargs['layer_type'],
                           activation=kwargs['activation']))
    layers.append(len(alpha))

    exp = theanets.Experiment(theanets.recurrent.Classifier, layers=layers)

    exp.train(
        batch,
        algo=kwargs['algorithm'],
        patience=kwargs['patience'],
        min_improvement=kwargs['min_improvement'],
        validate_every=kwargs['validate_every'],
        batch_size=kwargs['batch_size'],
        train_batches=kwargs['train_batches'],
        valid_batches=kwargs['valid_batches'],
        learning_rate=kwargs['learning_rate'],
        momentum=kwargs['momentum'],
        nesterov=kwargs['nesterov'],
        save_progress=kwargs['save_progress'],
        save_every=kwargs['save_every'],
        weight_l1=kwargs['weight_l1'],
        weight_l2=kwargs['weight_l2'],
        hidden_l2=kwargs['hidden_l2'],
        hidden_l1=kwargs['hidden_l1'],
        input_noise=kwargs['input_noise'],
        input_dropouts=kwargs['input_dropouts'],
        hidden_noise=kwargs['hidden_noise'],
        hidden_dropouts=kwargs['hidden_dropouts'],
    )

    if kwargs['save_progress']:
        exp.save(kwargs['save_progress'])


if __name__ == '__main__':
   main()
